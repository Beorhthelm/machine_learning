{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced ensembling techniques"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "8XgQKq6Ldl-g"
   },
   "source": [
    "In this project I am going to work with a dataset based on the following data:\n",
    "\n",
    "https://archive.ics.uci.edu/ml/datasets/Online+News+Popularity\n",
    "\n",
    "_Citation:_\n",
    "\n",
    "* _K. Fernandes, P. Vinagre and P. Cortez. A Proactive Intelligent Decision Support System for Predicting the Popularity of Online News. Proceedings of the 17th EPIA 2015 - Portuguese Conference on Artificial Intelligence, September, Coimbra, Portugal._\n",
    "\n",
    "The dataset contains the information about the internet news articles. I am going to predict a number of shares of the news article (target column: `shares`). The information about the features is available through the link above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "qx7Y3W-_3LPT"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from catboost import CatBoostRegressor\n",
    "from lightgbm import LGBMRegressor, early_stopping\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "ROnt-v3A5Rzy"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/OnlineNewsPopularity.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_tokens_title</th>\n",
       "      <th>n_tokens_content</th>\n",
       "      <th>n_unique_tokens</th>\n",
       "      <th>n_non_stop_words</th>\n",
       "      <th>n_non_stop_unique_tokens</th>\n",
       "      <th>num_hrefs</th>\n",
       "      <th>num_self_hrefs</th>\n",
       "      <th>num_imgs</th>\n",
       "      <th>num_videos</th>\n",
       "      <th>average_token_length</th>\n",
       "      <th>...</th>\n",
       "      <th>min_positive_polarity</th>\n",
       "      <th>max_positive_polarity</th>\n",
       "      <th>avg_negative_polarity</th>\n",
       "      <th>min_negative_polarity</th>\n",
       "      <th>max_negative_polarity</th>\n",
       "      <th>title_subjectivity</th>\n",
       "      <th>title_sentiment_polarity</th>\n",
       "      <th>abs_title_subjectivity</th>\n",
       "      <th>abs_title_sentiment_polarity</th>\n",
       "      <th>shares</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.980392</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.917808</td>\n",
       "      <td>...</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>-0.200000</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>-0.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10.0</td>\n",
       "      <td>1280.0</td>\n",
       "      <td>0.407072</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.572592</td>\n",
       "      <td>53.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.496875</td>\n",
       "      <td>...</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.396465</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.050000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9.0</td>\n",
       "      <td>576.0</td>\n",
       "      <td>0.535088</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.693182</td>\n",
       "      <td>9.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.850694</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.9</td>\n",
       "      <td>-0.352778</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9.0</td>\n",
       "      <td>210.0</td>\n",
       "      <td>0.583732</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.729927</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.804762</td>\n",
       "      <td>...</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.5</td>\n",
       "      <td>-0.250000</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13.0</td>\n",
       "      <td>1578.0</td>\n",
       "      <td>0.429864</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.624595</td>\n",
       "      <td>34.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.593790</td>\n",
       "      <td>...</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.332037</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.071429</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 59 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   n_tokens_title  n_tokens_content  n_unique_tokens  n_non_stop_words   \n",
       "0             6.0              73.0         0.916667               1.0  \\\n",
       "1            10.0            1280.0         0.407072               1.0   \n",
       "2             9.0             576.0         0.535088               1.0   \n",
       "3             9.0             210.0         0.583732               1.0   \n",
       "4            13.0            1578.0         0.429864               1.0   \n",
       "\n",
       "   n_non_stop_unique_tokens  num_hrefs  num_self_hrefs  num_imgs  num_videos   \n",
       "0                  0.980392        6.0             0.0       1.0         0.0  \\\n",
       "1                  0.572592       53.0             2.0      10.0         1.0   \n",
       "2                  0.693182        9.0             5.0       1.0         1.0   \n",
       "3                  0.729927        6.0             2.0       1.0         0.0   \n",
       "4                  0.624595       34.0            24.0      11.0         0.0   \n",
       "\n",
       "   average_token_length  ...  min_positive_polarity  max_positive_polarity   \n",
       "0              4.917808  ...               0.150000                    0.5  \\\n",
       "1              4.496875  ...               0.033333                    1.0   \n",
       "2              4.850694  ...               0.100000                    0.9   \n",
       "3              4.804762  ...               0.062500                    0.5   \n",
       "4              4.593790  ...               0.033333                    1.0   \n",
       "\n",
       "   avg_negative_polarity  min_negative_polarity  max_negative_polarity   \n",
       "0              -0.200000                   -0.2              -0.200000  \\\n",
       "1              -0.396465                   -1.0              -0.050000   \n",
       "2              -0.352778                   -1.0              -0.100000   \n",
       "3              -0.250000                   -0.4              -0.100000   \n",
       "4              -0.332037                   -1.0              -0.071429   \n",
       "\n",
       "   title_subjectivity  title_sentiment_polarity  abs_title_subjectivity   \n",
       "0            0.000000                  0.000000                0.500000  \\\n",
       "1            0.000000                  0.000000                0.500000   \n",
       "2            0.000000                  0.000000                0.500000   \n",
       "3            0.454545                  0.136364                0.045455   \n",
       "4            0.000000                  0.000000                0.500000   \n",
       "\n",
       "   abs_title_sentiment_polarity  shares  \n",
       "0                      0.000000     878  \n",
       "1                      0.000000    1100  \n",
       "2                      0.000000     872  \n",
       "3                      0.136364    5000  \n",
       "4                      0.000000    1600  \n",
       "\n",
       "[5 rows x 59 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "B-95PM9m56JZ"
   },
   "source": [
    "First of all, we can check if there are any missing values in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "AuyhBV3ULyIj"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum().sum()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "36BGDOx19peU"
   },
   "source": [
    "Let's separate the target from the dataframe with features.\n",
    "\n",
    "Next, let's split the data into train/val/test sets in the ratio 60:20:20. I will use train set to train the models, val set to validate them and test set to calculate the final error of the blend. So, test set will be a completely unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "x18Nt2Ln9hAQ"
   },
   "outputs": [],
   "source": [
    "X = df.drop('shares', axis=1)\n",
    "y = df['shares']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "JBpG87eiL7DY"
   },
   "outputs": [],
   "source": [
    "X_train, X_val_test, y_train, y_val_test = train_test_split(\n",
    "    X, y, test_size=0.4, random_state=13)\n",
    "\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_val_test, y_val_test, test_size=0.5, random_state=13)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "zijl0TGp_8P2"
   },
   "source": [
    "\n",
    "---\n",
    "\n",
    "Now let's train the first model - XGBoost. A link to the documentation: https://xgboost.readthedocs.io/en/latest/\n",
    "\n",
    "I will use Scikit-Learn Wrapper interface for XGBoost (and the same logic applies to the following LightGBM and CatBoost models). Here, we have a regression task - hence I will use `XGBRegressor`. The parameters of `XGBRegressor`: https://xgboost.readthedocs.io/en/latest/python/python_api.html#xgboost.XGBRegressor\n",
    "\n",
    "The main list of XGBoost parameters: https://xgboost.readthedocs.io/en/latest/parameter.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "bsf1mDiPL-dX"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10329.20768"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_reg = XGBRegressor(objective='reg:squarederror',\n",
    "                       n_estimators=200,\n",
    "                       max_depth=5,\n",
    "                       learning_rate=0.01,\n",
    "                       random_state=13)\n",
    "\n",
    "xgb_reg.fit(X_train, y_train)\n",
    "y_pred_val_xgb = xgb_reg.predict(X_val)\n",
    "\n",
    "round(np.sqrt(mean_squared_error(y_val, y_pred_val_xgb)), 5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "O9YFI8rhBTmN"
   },
   "source": [
    "I have decided to build 200 trees in the model. However, it is hard to understand whether it is a good decision.\n",
    "\n",
    "During the training process, it is possible to stop constructing the ensemble if the validation error does not decrease anymore. Let's train the same the same XGBoost model with `eval_set=[(X_val, y_val)]` (to evaluate the boosting model after building a new tree) and `early_stopping_rounds=50`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "WmX6FqvoMA4L"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8890.23464"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_reg = XGBRegressor(objective='reg:squarederror',\n",
    "                       n_estimators=200,\n",
    "                       max_depth=5,\n",
    "                       learning_rate=0.01,\n",
    "                       early_stopping_rounds=50,\n",
    "                       random_state=13)\n",
    "\n",
    "xgb_reg.fit(X_train, y_train,\n",
    "            eval_set=[(X_val, y_val)],\n",
    "            verbose=False)\n",
    "\n",
    "y_pred_val_xgb = xgb_reg.predict(X_val)\n",
    "\n",
    "round(np.sqrt(mean_squared_error(y_val, y_pred_val_xgb)), 5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "j_jy-BqrCaci"
   },
   "source": [
    "Notes on parameter tuning: https://xgboost.readthedocs.io/en/latest/tutorials/param_tuning.html\n",
    "\n",
    "Here, I tried to tune some parameters of the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "E4nDYySWMC_p"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8692.77417"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_reg = XGBRegressor(objective='reg:squarederror',\n",
    "                         n_estimators=5000,\n",
    "                         max_depth=4,\n",
    "                         learning_rate=0.001,\n",
    "                         gamma=1,\n",
    "                         subsample=0.5,\n",
    "                         early_stopping_rounds=500,\n",
    "                         random_state=13)\n",
    "\n",
    "xgb_reg.fit(X_train, y_train,\n",
    "            eval_set=[(X_val, y_val)],\n",
    "            verbose=False)\n",
    "\n",
    "y_pred_val_xgb = xgb_reg.predict(X_val)\n",
    "\n",
    "round(np.sqrt(mean_squared_error(y_val, y_pred_val_xgb)), 5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "57TAxOUfC_-9"
   },
   "source": [
    "Let's calculate feature importances according to the trained model and find the most important feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "uyWWYYvKMEwO"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data_channel_is_bus'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(zip(X_train.columns, xgb_reg.feature_importances_), key=lambda k: k[1])[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "C4EWKwRkEJGO"
   },
   "source": [
    "\n",
    "---\n",
    "\n",
    "Let's move to LightGBM. I will work with `LGBMRegressor`.\n",
    "\n",
    "LGBMRegressor parameters: https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.LGBMRegressor.html#lightgbm.LGBMRegressor\n",
    "\n",
    "The main list of LightGBM parameters: https://lightgbm.readthedocs.io/en/latest/Parameters.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "4Y-dWz4SMHTB"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8451.2859"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgbm_reg = LGBMRegressor(objective='regression',\n",
    "                         max_depth=5,\n",
    "                         learning_rate=0.01,\n",
    "                         n_estimators=200,\n",
    "                         random_state=13)\n",
    "\n",
    "lgbm_reg.fit(X_train, y_train,\n",
    "             eval_set=[(X_val, y_val)],\n",
    "             eval_metric='rmse',\n",
    "             callbacks=[early_stopping(stopping_rounds=50, verbose=False)])\n",
    "\n",
    "y_pred_val_lgbm = lgbm_reg.predict(X_val)\n",
    "\n",
    "round(np.sqrt(mean_squared_error(y_val, y_pred_val_lgbm)), 5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The speed of the algorithm is noticeably faster in comparison with the speed of XGBoost model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "w9CrcUB3GZg9"
   },
   "source": [
    "Notes on parameter tuning: https://lightgbm.readthedocs.io/en/latest/Parameters-Tuning.html\n",
    "\n",
    "Here, I tried to tune some parameters of the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "ybE86ky2MJLx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] num_threads is set=24, n_jobs=-1 will be ignored. Current value: num_threads=24\n",
      "[LightGBM] [Warning] lambda_l2 is set=1.0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8421.39852"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgbm_reg = LGBMRegressor(objective='regression',\n",
    "                         boosting_type='goss',\n",
    "                         max_depth=3,\n",
    "                         learning_rate=0.001,\n",
    "                         n_estimators=5000,\n",
    "                         lambda_l2=1.0,\n",
    "                         num_threads=24, # https://github.com/microsoft/LightGBM/issues/5441\n",
    "                         random_state=13)\n",
    "\n",
    "lgbm_reg.fit(X_train, y_train,\n",
    "             eval_set=[(X_val, y_val)],\n",
    "             eval_metric='rmse',\n",
    "             callbacks=[early_stopping(stopping_rounds=500, verbose=False)])\n",
    "\n",
    "y_pred_val_lgbm = lgbm_reg.predict(X_val)\n",
    "\n",
    "round(np.sqrt(mean_squared_error(y_val, y_pred_val_lgbm)), 5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "pRMPMVMqJEJu"
   },
   "source": [
    "Let's calculate feature importances according to the trained model and find the most important feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'self_reference_min_shares'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(zip(lgbm_reg.feature_name_, lgbm_reg.feature_importances_), key=lambda k: k[1])[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "Vjzix3YhSy5x"
   },
   "source": [
    "Since some features are not important for the model, we can drop them in order to try to construct a better model which does not consider them at all.\n",
    "\n",
    "Let's obtain new train and validation sets without the features with LightGBM importance less than 10 and train the same model on the new train set in the same manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "important_features = list(name for name, importance in zip(lgbm_reg.feature_name_, lgbm_reg.feature_importances_) if importance >= 10)\n",
    "\n",
    "X_train_new = X_train[important_features]\n",
    "X_val_new = X_val[important_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] num_threads is set=24, n_jobs=-1 will be ignored. Current value: num_threads=24\n",
      "[LightGBM] [Warning] lambda_l2 is set=1.0, reg_lambda=0.0 will be ignored. Current value: lambda_l2=1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8422.73009"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgbm_reg.fit(X_train_new, y_train,\n",
    "             eval_set=[(X_val_new, y_val)],\n",
    "             eval_metric='rmse',\n",
    "             callbacks=[early_stopping(stopping_rounds=500, verbose=False)])\n",
    "\n",
    "y_pred_val_lgbm = lgbm_reg.predict(X_val_new)\n",
    "\n",
    "round(np.sqrt(mean_squared_error(y_val, y_pred_val_lgbm)), 5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "5pIu6_n3JvGI"
   },
   "source": [
    "\n",
    "---\n",
    "\n",
    "Let's move to CatBoost. I will work with `CatBoostRegressor`.\n",
    "\n",
    "Info about `CatBoostRegressor`: https://catboost.ai/docs/concepts/python-reference_catboostregressor.html\n",
    "\n",
    "CatBoost parameters: https://catboost.ai/docs/concepts/python-reference_parameters-list.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "R-rFegFdMPx6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8485.01086"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_reg = CatBoostRegressor(loss_function='RMSE',\n",
    "                            iterations=200,\n",
    "                            learning_rate=0.01,\n",
    "                            max_depth=5,\n",
    "                            random_state=13)\n",
    "\n",
    "cat_reg.fit(X_train, y_train,\n",
    "            eval_set=[(X_val, y_val)],\n",
    "            early_stopping_rounds=50,\n",
    "            verbose=False)\n",
    "\n",
    "y_pred_val_cat = cat_reg.predict(X_val)\n",
    "\n",
    "round(np.sqrt(mean_squared_error(y_val, y_pred_val_cat)), 5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The speed of the algorithm is even faster in comparison with the speed of XGBoost and LightGBM models."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "keSiZN2DNYbn"
   },
   "source": [
    "Notes on parameter tuning: https://catboost.ai/docs/concepts/parameter-tuning.html\n",
    "\n",
    "Here, I tried to tune some parameters of the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "t-1QF1nKMRye"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8465.44037"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_reg = CatBoostRegressor(loss_function='RMSE',\n",
    "                            n_estimators=5000,\n",
    "                            learning_rate=0.001,\n",
    "                            max_depth=9,\n",
    "                            random_state=13)\n",
    "\n",
    "cat_reg.fit(X_train, y_train,\n",
    "            eval_set=[(X_val, y_val)],\n",
    "            early_stopping_rounds=500,\n",
    "            verbose=False)\n",
    "\n",
    "y_pred_val_cat = cat_reg.predict(X_val)\n",
    "\n",
    "round(np.sqrt(mean_squared_error(y_val, y_pred_val_cat)), 5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "4d0kLl7HS3lG"
   },
   "source": [
    "Let's calculate feature importances according to the trained model and find the most important feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "Z0-Bw-IbMVW3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'kw_avg_avg'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(zip(cat_reg.feature_names_, cat_reg.feature_importances_), key=lambda k: k[1])[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "MiMdjrX7TdSx"
   },
   "source": [
    "\n",
    "---\n",
    "\n",
    "Finally, let's take a `Lasso` model from `sklearn` and train it on the train set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "D-3Ie1OmMXbj"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8426.97894"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = Lasso(alpha=10.0, random_state=13)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred_val_lasso = clf.predict(X_val)\n",
    "\n",
    "round(np.sqrt(mean_squared_error(y_val, y_pred_val_lasso)), 5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "6COwSmbBt1m8"
   },
   "source": [
    "Let's compare the results on the validation set of the trained models:\n",
    "\n",
    "* XGBoost\n",
    "* LightGBM\n",
    "* CatBoost\n",
    "* Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'lgb'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models = ['xgb', 'lgb', 'cb', 'lasso']\n",
    "results = [y_pred_val_xgb, y_pred_val_lgbm, y_pred_val_cat, y_pred_val_lasso]\n",
    "accuracies = [np.sqrt(mean_squared_error(y_val, result)) for result in results]\n",
    "\n",
    "min(zip(models, accuracies), key=lambda x: x[1])[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, the LightGBM model has the best RMSE value on the validation set."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "1aEMdDB0cY01"
   },
   "source": [
    "\n",
    "---\n",
    "\n",
    "Finally, let's move to blending the models that we obtained. First, let's calculate the predictions for the trained models on the validation set and concatenate them into a single dataframe `X_val_blend`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {\n",
    "    'xgb': y_pred_val_xgb,\n",
    "    'lgb': y_pred_val_lgbm,\n",
    "    'cb': y_pred_val_cat,\n",
    "    'lasso': y_pred_val_lasso\n",
    "}\n",
    "\n",
    "X_val_blend = pd.DataFrame(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>xgb</th>\n",
       "      <th>lgb</th>\n",
       "      <th>cb</th>\n",
       "      <th>lasso</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2298.947754</td>\n",
       "      <td>3745.689944</td>\n",
       "      <td>3680.924182</td>\n",
       "      <td>4270.039931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3208.189209</td>\n",
       "      <td>5295.171499</td>\n",
       "      <td>4487.549790</td>\n",
       "      <td>6755.853939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1171.030029</td>\n",
       "      <td>2362.077406</td>\n",
       "      <td>2899.190806</td>\n",
       "      <td>960.707930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1715.524292</td>\n",
       "      <td>2979.686550</td>\n",
       "      <td>3102.992450</td>\n",
       "      <td>3280.292136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1780.428223</td>\n",
       "      <td>3058.667040</td>\n",
       "      <td>3404.989586</td>\n",
       "      <td>1586.863807</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           xgb          lgb           cb        lasso\n",
       "0  2298.947754  3745.689944  3680.924182  4270.039931\n",
       "1  3208.189209  5295.171499  4487.549790  6755.853939\n",
       "2  1171.030029  2362.077406  2899.190806   960.707930\n",
       "3  1715.524292  2979.686550  3102.992450  3280.292136\n",
       "4  1780.428223  3058.667040  3404.989586  1586.863807"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val_blend.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "RikEkjthVKX-"
   },
   "source": [
    "Let's obtain a matrix of pairwise Pearson Correlation Coefficient (PCC) values for the column of the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "e7wJfNuTMh4A"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>xgb</th>\n",
       "      <th>lgb</th>\n",
       "      <th>cb</th>\n",
       "      <th>lasso</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>xgb</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.633484</td>\n",
       "      <td>0.776625</td>\n",
       "      <td>0.481110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lgb</th>\n",
       "      <td>0.633484</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.844554</td>\n",
       "      <td>0.761926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cb</th>\n",
       "      <td>0.776625</td>\n",
       "      <td>0.844554</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.700703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lasso</th>\n",
       "      <td>0.481110</td>\n",
       "      <td>0.761926</td>\n",
       "      <td>0.700703</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            xgb       lgb        cb     lasso\n",
       "xgb    1.000000  0.633484  0.776625  0.481110\n",
       "lgb    0.633484  1.000000  0.844554  0.761926\n",
       "cb     0.776625  0.844554  1.000000  0.700703\n",
       "lasso  0.481110  0.761926  0.700703  1.000000"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val_blend.corr()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "oKrXVxTGVaZa"
   },
   "source": [
    "Let's blend models into the ensemble with the weights 0.25, 0.25, 0.25 and 0.25 (just mean value of the predictions).\n",
    "\n",
    "Compare it with RMSE of each model and think whether this is a good ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "NoD2EsNRMjvQ"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8439.26508"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(np.sqrt(mean_squared_error(y_val, X_val_blend.mean(axis=1))), 5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RMSE of such ensemble calculated on the validation set is quite similar to the RMSE of each separate model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "EBPbgfyadERE"
   },
   "source": [
    "Let's tune the weights of the ensemble. In order to do that, let's run each model weight through `np.linspace(0, 1, 101)`, so that all possible values of each weight will be [0.0, 0.01, ..., 0.99, 1.0]. So, we skip each combinations of weights, if their sum is not equal to 1.0. If the sum of the weights in the combination is equal to 1.0, though, we get ensemble prediction on the validation set using these weights and calculate RMSE value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import permutations\n",
    "\n",
    "\n",
    "weights_list = [np.array(weights) for weights in permutations(np.linspace(0, 1, 101), 4) if sum(weights) == 1.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.  , 0.52, 0.01, 0.47]), 8405.82118305431)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights_rmse_list = []\n",
    "\n",
    "for weights in weights_list:\n",
    "    weights_rmse_list.append((\n",
    "        weights,\n",
    "        np.sqrt(mean_squared_error(y_val, np.sum(X_val_blend * weights, axis=1)))\n",
    "        ))\n",
    "\n",
    "min(weights_rmse_list, key=lambda x: x[1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's select a combination of weights with the best RMSE value - these are the best weights for the ensemble. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_weights = min(weights_rmse_list, key=lambda x: x[1])[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the RMSE value of the ensemble is better than the RMSE values of the separate models in it."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "IFqDAt-EdKxS"
   },
   "source": [
    "Using the best weights obtained above, let's run the best ensemble on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "dEW1M-s6MtOx"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8445.30991"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_d = {\n",
    "    'xgb': xgb_reg.predict(X_test),\n",
    "    'lgb': lgbm_reg.predict(X_test[important_features]),\n",
    "    'cb': cat_reg.predict(X_test),\n",
    "    'lasso': clf.predict(X_test)\n",
    "}\n",
    "X_test_blend = pd.DataFrame(test_d)\n",
    "\n",
    "round(np.sqrt(mean_squared_error(y_test, np.sum(X_test_blend * best_weights, axis=1))), 5)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyM8xzctGVVD8ZayBsxVM3cd",
   "collapsed_sections": [],
   "name": "Week2_practice.ipynb",
   "provenance": []
  },
  "coursera": {
   "schema_names": [
    "ensembling-techniques-task-week-2"
   ]
  },
  "kernelspec": {
   "display_name": "machine_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "vscode": {
   "interpreter": {
    "hash": "5d9a420dd7d9958c9bfab5a33c6b672d78ee78f6ceacdd75e61df641f7a5961d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
